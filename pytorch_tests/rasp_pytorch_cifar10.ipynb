{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42bdbc49-e10a-4c0f-8d0a-38983c5a320f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f2d8ebd64734c3e93ae7d97ffaa8930",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "--------\n",
      "Train loss: 1.63007 | Train acc: 40.13%\n",
      "\n",
      "Accuracy on all data: 0.5449000000953674\n",
      "Precision on all data: 0.5950759649276733\n",
      "recall on all data: 0.5449000597000122\n",
      "f1score on all data: 0.5563617944717407\n",
      "\n",
      "Test loss: 1.29398 | Test acc: 54.49%\n",
      "\n",
      "Epoch: 1\n",
      "--------\n",
      "Train loss: 1.06253 | Train acc: 62.85%\n",
      "\n",
      "Accuracy on all data: 0.7113999724388123\n",
      "Precision on all data: 0.7182292938232422\n",
      "recall on all data: 0.7113999724388123\n",
      "f1score on all data: 0.7119972109794617\n",
      "\n",
      "Test loss: 0.84040 | Test acc: 71.14%\n",
      "\n",
      "Epoch: 2\n",
      "--------\n",
      "Train loss: 0.79999 | Train acc: 72.43%\n",
      "\n",
      "Accuracy on all data: 0.7379999756813049\n",
      "Precision on all data: 0.7397554516792297\n",
      "recall on all data: 0.7380000352859497\n",
      "f1score on all data: 0.7347636222839355\n",
      "\n",
      "Test loss: 0.78399 | Test acc: 73.80%\n",
      "\n",
      "Epoch: 3\n",
      "--------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 213\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(epochs)):\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 213\u001b[0m     \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m               \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m               \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m               \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[43m               \u001b[49m\u001b[43maccuracy_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccuracy_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m               \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    219\u001b[0m     test_step(model\u001b[38;5;241m=\u001b[39mmodel_1,\n\u001b[0;32m    220\u001b[0m               data_loader\u001b[38;5;241m=\u001b[39mtest_dataloader,\n\u001b[0;32m    221\u001b[0m               loss_fn\u001b[38;5;241m=\u001b[39mloss_fn,\n\u001b[0;32m    222\u001b[0m               accuracy_fn\u001b[38;5;241m=\u001b[39maccuracy_fn,\n\u001b[0;32m    223\u001b[0m               device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m    225\u001b[0m train_time_end \u001b[38;5;241m=\u001b[39m timer()\n",
      "Cell \u001b[1;32mIn[5], line 120\u001b[0m, in \u001b[0;36mtrain_step\u001b[1;34m(model, data_loader, loss_fn, optimizer, accuracy_fn, device)\u001b[0m\n\u001b[0;32m    117\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    119\u001b[0m \u001b[38;5;66;03m# 4. Loss backward\u001b[39;00m\n\u001b[1;32m--> 120\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# 5. Optimizer step (update the model's parameters once *per batch*)\u001b[39;00m\n\u001b[0;32m    123\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Computer vision using pytorch\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from timeit import default_timer as timer\n",
    "import torchmetrics.classification as mtr\n",
    "# Import matplotlib for visualization\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# Import tqdm for progress bar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "# transform_train = transforms.Compose([\n",
    "#     transforms.RandomCrop(32, padding=4),\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "# ])\n",
    "\n",
    "# transform_test = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "# ])\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), \n",
    "                                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))])\n",
    "\n",
    "train_data = datasets.CIFAR10(\n",
    "    root=\"../datasets/data\", # where to download data to?\n",
    "    train=True, # do we want the training dataset?\n",
    "    download=True, # do we want to download yes/no?\n",
    "    transform=transform # how do we want to transform the data?\n",
    ")\n",
    "\n",
    "test_data = datasets.CIFAR10(\n",
    "    root=\"../datasets/data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "class_names = train_data.classes\n",
    "\n",
    "# Setup the batch size hyperparameter\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "# Turn datasets into iterables (batches)\n",
    "train_dataloader = DataLoader(dataset=train_data,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              shuffle=True,\n",
    "                              num_workers=2)\n",
    "\n",
    "test_dataloader = DataLoader(dataset=test_data,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             shuffle=False,\n",
    "                             num_workers=2)\n",
    "\n",
    "\n",
    "# model architechture\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=48, kernel_size=(3,3), padding=(1,1))\n",
    "        self.conv2 = nn.Conv2d(in_channels=48, out_channels=96, kernel_size=(3,3), padding=(1,1))\n",
    "        self.conv3 = nn.Conv2d(in_channels=96, out_channels=192, kernel_size=(3,3), padding=(1,1))\n",
    "        self.conv4 = nn.Conv2d(in_channels=192, out_channels=256, kernel_size=(3,3), padding=(1,1))\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.fc1 = nn.Linear(in_features=8*8*256, out_features=512)\n",
    "        self.fc2 = nn.Linear(in_features=512, out_features=64)\n",
    "        self.Dropout = nn.Dropout(0.25)\n",
    "        self.fc3 = nn.Linear(in_features=64, out_features=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x)) #32*32*48\n",
    "        x = F.relu(self.conv2(x)) #32*32*96\n",
    "        x = self.pool(x) #16*16*96\n",
    "        x = self.Dropout(x)\n",
    "        x = F.relu(self.conv3(x)) #16*16*192\n",
    "        x = F.relu(self.conv4(x)) #16*16*256\n",
    "        x = self.pool(x) # 8*8*256\n",
    "        x = self.Dropout(x)\n",
    "        x = x.view(-1, 8*8*256) # reshape x\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.Dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "def train_step(model: torch.nn.Module,\n",
    "               data_loader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               accuracy_fn,\n",
    "               device: torch.device = device):\n",
    "    \"\"\"Performs a training with model trying to learn on data_loader.\"\"\"\n",
    "    train_loss, train_acc = 0, 0\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(data_loader):\n",
    "        # Put data in target device\n",
    "        X, y = X.to(device), y.clone().detach().long().to(device)\n",
    "        # 1. Forward pass\n",
    "        y_pred = model(X)\n",
    "\n",
    "        # 2. Calculate loss (per batch)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss # accumulate train loss\n",
    "        train_acc += accuracy_fn(y_true=y,\n",
    "                                 y_pred=y_pred.argmax(dim=1))\n",
    "\n",
    "        # 3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 4. Loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. Optimizer step (update the model's parameters once *per batch*)\n",
    "        optimizer.step()\n",
    "\n",
    "    # Divide total train loss by length of train dataloader\n",
    "    train_loss /= len(data_loader)\n",
    "    train_acc /= len(data_loader)\n",
    "    print(f\"Train loss: {train_loss:.5f} | Train acc: {train_acc:.2f}%\\n\")\n",
    "\n",
    "def test_step(model: torch.nn.Module,\n",
    "               data_loader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               accuracy_fn,\n",
    "               device: torch.device = device):\n",
    "    \"\"\"Performs a testing loop step on model going over data_loader.\"\"\"\n",
    "    \n",
    "    # initialize metric\n",
    "\n",
    "    acc_metric = mtr.Accuracy(task=\"multiclass\", num_classes=len(class_names))\n",
    "    precision_metric = mtr.Precision(task=\"multiclass\", average='macro', num_classes=len(class_names))\n",
    "    recall_metric = mtr.Recall(task=\"multiclass\", average='macro', num_classes=len(class_names))\n",
    "    f1_score_metric = mtr.F1Score(task=\"multiclass\", average='macro', num_classes=len(class_names))\n",
    "\n",
    "    test_loss, test_acc = 0, 0\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for batch, (X, y) in enumerate(data_loader):\n",
    "            # Put data in target device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            # 1. Forward pass (output raw logits)\n",
    "            y_pred = model(X)\n",
    "\n",
    "            # 2. Calculate loss (per batch)\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            test_loss += loss # accumulate train loss\n",
    "            test_acc += accuracy_fn(y_true=y,\n",
    "                                     y_pred=y_pred.argmax(dim=1))\n",
    "            # metric on current batch\n",
    "            acc = acc_metric(y_pred.cpu(), y.cpu())\n",
    "            precision = precision_metric(y_pred.cpu(), y.cpu())\n",
    "            recall = recall_metric(y_pred.cpu(), y.cpu())\n",
    "            f1score = f1_score_metric(y_pred.cpu(), y.cpu())\n",
    "            \n",
    "\n",
    "        # Divide total train loss by length of train dataloader to get the average\n",
    "        test_loss /= len(data_loader)\n",
    "        test_acc /= len(data_loader)\n",
    "        # metric on all batches using custom accumulation\n",
    "        acc = acc_metric.compute()\n",
    "        print(f\"Accuracy on all data: {acc}\")\n",
    "        precision = precision_metric.compute()\n",
    "        print(f\"Precision on all data: {precision}\")\n",
    "        recall = recall_metric.compute()\n",
    "        print(f\"recall on all data: {recall}\")\n",
    "        f1score = f1_score_metric.compute()\n",
    "        print(f\"f1score on all data: {f1score}\")\n",
    "\n",
    "        # Resetting internal state such that metric ready for new data\n",
    "        acc_metric.reset()\n",
    "        precision_metric.reset()\n",
    "        recall_metric.reset()\n",
    "        f1_score_metric.reset()\n",
    "        print(f\"\\nTest loss: {test_loss:.5f} | Test acc: {test_acc:.2f}%\\n\")\n",
    "\n",
    "# Calculate accuracy (a classification metric)\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc\n",
    "\n",
    "def print_train_time(start: float,\n",
    "                     end: float,\n",
    "                     device: torch.device = None):\n",
    "  \"\"\"Prints difference between start and end time.\"\"\"\n",
    "  total_time = end - start\n",
    "  print(f\"Train time on {device}: {total_time:.3f} seconds\")\n",
    "  return total_time\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "model_1 = ConvNet().to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(params=model_1.parameters(), lr= 0.01)\n",
    "\n",
    "epochs = 25\n",
    "\n",
    "train_time_start = timer()\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f\"Epoch: {epoch}\\n--------\")\n",
    "    train_step(model=model_1,\n",
    "               data_loader=train_dataloader,\n",
    "               loss_fn=loss_fn,\n",
    "               optimizer=optimizer,\n",
    "               accuracy_fn=accuracy_fn,\n",
    "               device=device)\n",
    "    test_step(model=model_1,\n",
    "              data_loader=test_dataloader,\n",
    "              loss_fn=loss_fn,\n",
    "              accuracy_fn=accuracy_fn,\n",
    "              device=device)\n",
    "\n",
    "train_time_end = timer()\n",
    "total_train_time_model = print_train_time(start=train_time_start,\n",
    "                                            end=train_time_end,\n",
    "                                            device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b417d34-dcbd-48e8-b8bd-53fafcef3cfc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
